import pandas as pd
import torch
from torch.utils.data import Dataset
import re
import string
from collections import Counter
from transformers import AutoTokenizer
import numpy as np

class SentimentDataset(Dataset):
    def __init__(self, csv_path, model_type='rnn', max_length=512, vocab_size=10000, test_mode=False):
        """
        ê°ì • ë¶„ì„ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
        
        Args:
            csv_path (str): CSV íŒŒì¼ ê²½ë¡œ
            model_type (str): ëª¨ë¸ íƒ€ì… ('rnn', 'lstm', 'gpt', 'bert')
            max_length (int): ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
            vocab_size (int): ì–´íœ˜ ì‚¬ì „ í¬ê¸° (RNN/LSTMìš©)
            test_mode (bool): í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì—¬ë¶€
        """
        self.model_type = model_type.lower()
        self.max_length = max_length
        self.vocab_size = vocab_size
        self.test_mode = test_mode
        
        # ë°ì´í„° ë¡œë“œ
        self.data = pd.read_csv(csv_path)
        self.texts = self.data['text'].tolist()
        self.labels = torch.tensor(self.data['sentiment'].tolist(), dtype=torch.long)
        
        # ëª¨ë¸ë³„ ì „ì²˜ë¦¬ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        if self.model_type in ['rnn', 'lstm']:
            self._setup_traditional_models()
        elif self.model_type == 'gpt':
            self._setup_gpt_model()
        elif self.model_type == 'bert':
            self._setup_bert_model()
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
    
    def _basic_text_preprocessing(self, text):
        """ê°œì„ ëœ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ê°ì • ì •ë³´ ë³´ì¡´"""
        # ì†Œë¬¸ì ë³€í™˜
        text = text.lower()
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # URL ì œê±°
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # ì´ë©”ì¼ ì£¼ì†Œ ì œê±°
        text = re.sub(r'\S+@\S+', '', text)
        
        # ê°ì • í‘œí˜„ ë³´ì¡´ - 3ê°œê¹Œì§€ í—ˆìš© (sooooo â†’ soo)
        text = re.sub(r'(.)\1{3,}', r'\1\1\1', text)
        
        # ì´ëª¨í‹°ì½˜ì„ íŠ¹ìˆ˜ í† í°ìœ¼ë¡œ ë³€í™˜ (ì„ íƒì )
        emoji_pattern = r'[ğŸ˜€-ğŸ™ğŸ’€-ğŸŸ¿]'
        text = re.sub(emoji_pattern, ' <EMOJI> ', text)
        
        # ìˆ«ìë¥¼ ë§¥ë½ì— ë”°ë¼ ì²˜ë¦¬
        # ì ìˆ˜ë‚˜ í‰ì  ê°™ì€ ì¤‘ìš”í•œ ìˆ«ìëŠ” ë³´ì¡´
        text = re.sub(r'\b\d{5,}\b', '<NUM>', text)  # ê¸´ ìˆ«ìë§Œ í† í°í™”
        
        # ëª¨ë¸ë³„ êµ¬ë‘ì  ì²˜ë¦¬
        if self.model_type in ['rnn', 'lstm']:
            # ê°ì • í‘œí˜„ì— ì¤‘ìš”í•œ êµ¬ë‘ì  ë³´ì¡´
            text = re.sub(r'[^\w\s!?.,;:\'"-]', ' ', text)
            # ì—°ì†ëœ êµ¬ë‘ì  ì •ê·œí™”
            text = re.sub(r'[!]{2,}', '!!!', text)
            text = re.sub(r'[?]{2,}', '???', text)
        
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def _setup_traditional_models(self):
        """RNN, LSTMì„ ìœ„í•œ ê°œì„ ëœ ì„¤ì •"""
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_texts = []
        for text in self.texts:
            processed_text = self._basic_text_preprocessing(text)
            # ê°ì • í‘œí˜„ì— ì¤‘ìš”í•œ êµ¬ë‘ì ì€ ë³´ì¡´
            # ì™„ì „ ì œê±°í•˜ì§€ ì•Šê³  ì¼ë¶€ êµ¬ë‘ì ì€ ìœ ì§€
            processed_texts.append(processed_text)
        
        self.processed_texts = processed_texts
        
        if not self.test_mode:
            # ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•
            self._build_vocabulary()
        else:
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” ë‚˜ì¤‘ì— ì„¤ì •ë¨
            self.word2idx = None
            self.vocab_size_actual = None
        
        # í…ìŠ¤íŠ¸ë¥¼ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (ì–´íœ˜ì‚¬ì „ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if hasattr(self, 'word2idx') and self.word2idx is not None:
            self.encoded_texts = self._encode_texts()
        else:
            self.encoded_texts = None
    
    def _build_vocabulary(self):
        """ì–´íœ˜ ì‚¬ì „ êµ¬ì¶• (RNN/LSTMìš©)"""
        # ëª¨ë“  ë‹¨ì–´ ìˆ˜ì§‘
        all_words = []
        for text in self.processed_texts:
            all_words.extend(text.split())
        
        # ë¹ˆë„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì–´íœ˜ ì„ íƒ
        word_counts = Counter(all_words)
        most_common = word_counts.most_common(self.vocab_size - 4)  # íŠ¹ìˆ˜ í† í° 4ê°œ ì œì™¸
        
        # ì–´íœ˜ ì‚¬ì „ ìƒì„±
        self.word2idx = {
            '<PAD>': 0,
            '<UNK>': 1,
            '<SOS>': 2,
            '<EOS>': 3
        }
        
        for word, _ in most_common:
            self.word2idx[word] = len(self.word2idx)
        
        self.idx2word = {idx: word for word, idx in self.word2idx.items()}
        self.vocab_size_actual = len(self.word2idx)
    
    def _encode_texts(self):
        """í…ìŠ¤íŠ¸ë¥¼ ì¸ë±ìŠ¤ë¡œ ì¸ì½”ë”© (RNN/LSTMìš©)"""
        if self.word2idx is None:
            raise ValueError("word2idx is not initialized. Make sure vocabulary is built or transferred from training dataset.")
            
        encoded_texts = []
        for text in self.processed_texts:
            words = text.split()
            indices = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in words]
            
            # íŒ¨ë”© ì²˜ë¦¬
            if len(indices) > self.max_length:
                indices = indices[:self.max_length]
            else:
                indices.extend([self.word2idx['<PAD>']] * (self.max_length - len(indices)))
            
            encoded_texts.append(indices)
        
        return torch.tensor(encoded_texts, dtype=torch.long)
    
    def set_vocabulary(self, word2idx, vocab_size_actual):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìš© ì–´íœ˜ì‚¬ì „ ì„¤ì •"""
        self.word2idx = word2idx
        self.vocab_size_actual = vocab_size_actual
        if self.model_type in ['rnn', 'lstm']:
            self.encoded_texts = self._encode_texts()
    
    def _setup_gpt_model(self):
        """GPTë¥¼ ìœ„í•œ ì„¤ì •"""
        # GPT-2 í† í¬ë‚˜ì´ì € ì‚¬ìš©
        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ê°ì • ë ˆì´ë¸”ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ë¡œ êµ¬ì„±
        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (êµ¬ë‘ì  ë³´ì¡´ - GPTëŠ” ìì—°ì–´ ì´í•´ê°€ ì¢‹ìŒ)
        processed_texts = []
        for text in self.texts:
            processed_text = self._basic_text_preprocessing(text)
            # GPTìš© í”„ë¡¬í”„íŠ¸ í˜•íƒœë¡œ êµ¬ì„±
            prompt = f"Review: {processed_text} Sentiment:"
            processed_texts.append(prompt)
        
        self.processed_texts = processed_texts
        
        # í† í¬ë‚˜ì´ì§•
        self.encoded_texts = self.tokenizer(
            self.processed_texts,
            truncation=True,
            padding=True,
            max_length=self.max_length,
            return_tensors='pt'
        )
    
    def _setup_bert_model(self):
        """BERTë¥¼ ìœ„í•œ ì„¤ì •"""
        # BERT í† í¬ë‚˜ì´ì € ì‚¬ìš©
        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (BERTëŠ” subword í† í¬ë‚˜ì´ì§•ì„ í•˜ë¯€ë¡œ êµ¬ë‘ì  ë³´ì¡´)
        processed_texts = []
        for text in self.texts:
            processed_text = self._basic_text_preprocessing(text)
            processed_texts.append(processed_text)
        
        self.processed_texts = processed_texts
        
        # í† í¬ë‚˜ì´ì§•
        self.encoded_texts = self.tokenizer(
            self.processed_texts,
            truncation=True,
            padding=True,
            max_length=self.max_length,
            return_tensors='pt'
        )
    
    def get_vocab_size(self):
        """ì–´íœ˜ ì‚¬ì „ í¬ê¸° ë°˜í™˜"""
        if self.model_type in ['rnn', 'lstm']:
            return self.vocab_size_actual
        elif self.model_type == 'gpt':
            return self.tokenizer.vocab_size
        elif self.model_type == 'bert':
            return self.tokenizer.vocab_size
    
    def get_word2idx(self):
        """ë‹¨ì–´-ì¸ë±ìŠ¤ ë§¤í•‘ ë°˜í™˜ (RNN/LSTMìš©)"""
        if self.model_type in ['rnn', 'lstm']:
            return self.word2idx
        else:
            return None
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        if self.model_type in ['rnn', 'lstm']:
            return {
                'input_ids': self.encoded_texts[idx],
                'labels': self.labels[idx]
            }
        
        elif self.model_type == 'gpt':
            return {
                'input_ids': self.encoded_texts['input_ids'][idx],
                'attention_mask': self.encoded_texts['attention_mask'][idx],
                'labels': self.labels[idx]
            }
        
        elif self.model_type == 'bert':
            return {
                'input_ids': self.encoded_texts['input_ids'][idx],
                'attention_mask': self.encoded_texts['attention_mask'][idx],
                'token_type_ids': self.encoded_texts['token_type_ids'][idx],
                'labels': self.labels[idx]
            }
    
    def get_class_weights(self):
        """í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        class_counts = torch.bincount(self.labels)
        total_samples = len(self.labels)
        class_weights = total_samples / (len(class_counts) * class_counts.float())
        return class_weights
    
    def get_statistics(self):
        """ë°ì´í„°ì…‹ í†µê³„ ì •ë³´ ë°˜í™˜"""
        text_lengths = [len(text.split()) for text in self.processed_texts]
        class_counts = torch.bincount(self.labels)
        
        stats = {
            'total_samples': len(self.texts),
            'avg_text_length': np.mean(text_lengths),
            'max_text_length': np.max(text_lengths),
            'min_text_length': np.min(text_lengths),
            'class_distribution': {
                'negative': class_counts[0].item(),
                'neutral': class_counts[1].item(),
                'positive': class_counts[2].item()
            }
        }
        
        if self.model_type in ['rnn', 'lstm']:
            stats['vocab_size'] = self.vocab_size_actual
        
        return stats